{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential learning\n",
    "\n",
    "## Theory\n",
    "\n",
    "Imagine a scenario where the data comes to us in batches. For instance, we observe new user sessions each day; or, alternatively, from time to time a new portion of a dataset is labelled by judges.\n",
    "\n",
    "How can we update our current model to account for the incoming batch? One way is to disregard the existing model, add the new batch to the old dataset, and learn a new model from scratch using all the data. However, sometimes that approach is not cost-effective, and we look for a way to quickly update the existing model. How can we do that? Easy, Laplace approximation to the rescue!\n",
    "\n",
    "### Bayesian view\n",
    "\n",
    "We can think of that in a Bayesian setting: as a result of earlier learning, we obtained a Gaussian approximation $p_{old}(\\theta)$ for the parameter's $\\theta$ posterior. The next step is to update this distribution given a new evidence.\n",
    "$$\n",
    "p(\\theta) \\propto p_{old}(\\theta) P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "Taking logarithms of the both parts, we get\n",
    "$$\n",
    "log P(\\theta) =  log P(\\theta_{old}) + log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "To find the MAP estimate, we again find the maximizer of the posterior,\n",
    "$$\n",
    "\\hat \\theta_{new} = argmax_{\\theta} L_{new} = argmax_{\\theta} log P(\\theta_{old}) + log P(\\mathbb{D}_{new} | \\theta) = \n",
    "argmax_{\\theta} \\left[ \\frac{1}{2}(\\theta - \\hat \\theta)^T \\mathbb{H}_d (\\theta - \\hat \\theta) + log P(\\mathbb{D}_{new} | \\theta) \\right]\n",
    "$$\n",
    "By optimizing this equation numerically, we can find the updated model $\\theta_{new}$. Furthermore, we find can find the Laplace approximation of the posterior by analysing second-order derivatives around the optimum $\\hat \\theta_{new}$:\n",
    "$$\n",
    "log P(\\theta) \\sim const + \\frac{1}{2}(\\theta - \\hat \\theta_{new})^T \\left[ \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} \\right]  (\\theta - \\hat \\theta_{new})\n",
    "$$\n",
    "This time, $\\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} $ would be as follows:\n",
    "$$\n",
    "\\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} = \\mathbb{H}_d + \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "\n",
    "### Optimization view\n",
    "\n",
    "In an alternative point of view, we have a quadratic approximation of the loss on the \"old\" dataset, and can just optimize it jointly with a log-likelihood on the new incoming batch:\n",
    "$$\n",
    "L_{new} = const + \\frac{1}{2}(\\theta - \\hat \\theta)^T \\mathbb{H} (\\theta - \\hat \\theta) + log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "$$\n",
    "\\hat \\theta_{new} = argmax L_{new}\n",
    "$$\n",
    "and then we can replace this joint loss (on the incoming data and the surrogate representative of the loss on the \"old\" dataset) by a new quadratic surrogate, so that it can be used later.\n",
    "\n",
    "$$\n",
    "L_{new}(\\theta) \\approx L_{new}(\\hat \\theta_{new}) + \\frac{1}{2}(\\theta - \\hat \\theta_{new})^T \\left[ \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} \\right](\\theta - \\hat \\theta_{new})\n",
    "$$\n",
    "As earlier, we have\n",
    "$$\n",
    "\\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} = \\mathbb{H}_d + \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Illustration\n",
    "\n",
    "Let's illustrate the described approach in a following experiment. We will take the MNIST dataset and split its training subset in 10 equal parts. We will consider these subsets as batches of data that arrive sequentially: we start with the first batch, then the second arrives, etc.\n",
    "\n",
    "After that, we will learn on these parts in three scenarios:\n",
    " * in the `Naive updates` scenario we only learn on the freshly arrived data,\n",
    " * in the `Laplace updates` we use the sequential learning technique described above,\n",
    " * in the `Upper bound` scenario we use the entire training dataset in a single optimization problem.\n",
    "\n",
    "In the first two scenarios, we control how the error on the test set changes after observing a new batch of data.\n",
    "\n",
    "Note, that on each step, `Naive updates` and `Laplace updates` operate only with the incoming batch. Hence, they have roughly the same computational complexity/storage requirements, which is typically proportional to the number of the datapoints. On the other hand, the `Upper bound` operates with the dataset which is considerably larger. Due to that, `Upper bound` is expected to perform better than the other two optimization scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LaplacedModel, get_learning_curve\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Loading the MNIST dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "sequential_tasks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning task # 0\n",
      "validation loss: [0.5693]\n",
      "learning task # 1\n",
      "validation loss: [0.69580001]\n",
      "learning task # 2\n",
      "validation loss: [0.76959997]\n",
      "learning task # 3\n",
      "validation loss: [0.82069999]\n",
      "learning task # 4\n",
      "validation loss: [0.83359998]\n",
      "learning task # 5\n",
      "validation loss: [0.8387]\n",
      "learning task # 6\n",
      "validation loss: [0.85110003]\n",
      "learning task # 7\n",
      "validation loss: [0.85439998]\n",
      "learning task # 8\n",
      "validation loss: [0.85189998]\n",
      "learning task # 9\n",
      "validation loss: [0.85970002]\n"
     ]
    }
   ],
   "source": [
    "# We create a 784->50->10 multilayer network with a softmax output layer predicting a digit\n",
    "# the optimization would be performed using minibatches of size 4, no L2 regularization; \n",
    "# with the Laplace updates enabled\n",
    "\n",
    "model_laplaced = LaplacedModel(4, True)\n",
    "laplaced_loss_curve = get_learning_curve(model_laplaced, mnist.train, mnist.test, sequential_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5693], [0.69580001], [0.76959997], [0.82069999], [0.83359998], [0.8387], [0.85110003], [0.85439998], [0.85189998], [0.85970002]]\n"
     ]
    }
   ],
   "source": [
    "# Let's print the trajectory of the test losses to make sure the test loss is roughly decreasing\n",
    "print laplaced_loss_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning task # 0\n",
      "validation loss: [0.5693]\n",
      "learning task # 1\n",
      "validation loss: [0.57279998]\n",
      "learning task # 2\n",
      "validation loss: [0.58310002]\n",
      "learning task # 3\n",
      "validation loss: [0.5988]\n",
      "learning task # 4\n",
      "validation loss: [0.59689999]\n",
      "learning task # 5\n",
      "validation loss: [0.56989998]\n",
      "learning task # 6\n",
      "validation loss: [0.5959]\n",
      "learning task # 7\n",
      "validation loss: [0.6164]\n",
      "learning task # 8\n",
      "validation loss: [0.59100002]\n",
      "learning task # 9\n",
      "validation loss: [0.6401]\n"
     ]
    }
   ],
   "source": [
    "# Now we train the naive model: same parameters, but the Laplace updates are off\n",
    "model_naive = LaplacedModel(4, False)\n",
    "naive_loss_curve = get_learning_curve(model_naive, mnist.train, mnist.test, sequential_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5693], [0.57279998], [0.58310002], [0.5988], [0.59689999], [0.56989998], [0.5959], [0.6164], [0.59100002], [0.6401]]\n"
     ]
    }
   ],
   "source": [
    "# Again, we print the trajectory of the test losses - this time they \n",
    "# not really decrease - as we only use 1/10 th of the data each time!\n",
    "print naive_loss_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound_model = LaplacedModel(4, False)\n",
    "upper_bound_curve = upper_bound_model.learn_new_task(mnist.train, mnist.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.926]\n"
     ]
    }
   ],
   "source": [
    "print upper_bound_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1305be850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1302adb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(naive_loss_curve, label='Naive updates')\n",
    "plt.plot(laplaced_loss_curve, label='Laplace updates')\n",
    "plt.plot([upper_bound_curve for _ in naive_loss_curve], label='Upper bound')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "While the updated learning is not as good as the upper bound, we see it's way better than the naive approach. Moreover, after observing the entire dataset, it get pretty close to the good solution. Hence, it's a nice alternative when the it is infeasible/too expensive to learn on the entire dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
