{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sequential learning\n",
    "Let us consider a scenario when the data comes to us in chunks. For instance, we observe new user sessions each day; or, alternatively, from time to time a new portion of a dataset is labelled by humans.\n",
    "\n",
    "How can we update our current model to account for the incoming chunk? One way is disregard the existing model, add the new chunk to the earlier available dataset, and learn a new model from scratch. However, sometimes that approach is not cost-effective, and we look for a way to quickly update the existing model. How can we do that? Easy, Laplace approximation to the rescue.\n",
    "\n",
    "\n",
    "We can think of that in a Bayesian setting: as a result of earlier learning, we obtained a Gaussian approximation $p_{old}(\\theta)$ for the parameter $\\theta$. The next step is to update this distribution given a new evidence.\n",
    "$$\n",
    "p(\\theta) \\propto p_{old}(\\theta) P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "Taking logarithms of the both parts, we get\n",
    "$$\n",
    "log P(\\theta) =  log P(\\theta_{old}) + log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "Maximizing posterior w.r.t. $\\theta$,\n",
    "$$\n",
    "\\hat \\theta_{new} = argmax_{\\theta} L_{new} = argmax_{\\theta} ~ log P(\\theta_{old}) + log P(\\mathbb{D}_{new} | \\theta) = \n",
    "argmax_{\\theta} \\left[ \\frac{1}{2}(\\theta - \\hat \\theta)^T \\mathbb{H}_d (\\theta - \\hat \\theta) + log P(\\mathbb{D}_{new} | \\theta) \\right]\n",
    "$$\n",
    "By optimizing this equation numerically, we can find the updated model $\\theta_{new}$. Furthermore, we find can find the Laplace approximation of the posterior by analysing second-order derivatives around the optimum $\\hat \\theta_{new}$:\n",
    "$$\n",
    "log P(\\theta) \\sim const + \\frac{1}{2}(\\theta - \\hat \\theta_{new})^T \\left[ \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} \\right]  (\\theta - \\hat \\theta_{new})\n",
    "$$\n",
    "This time, $\\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} $ would be as follows:\n",
    "$$\n",
    "\\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} = \\mathbb{H}_d + \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "In an alternative point of view, we have a quadratic approximation of the loss on the \"old\" dataset, and can just optimize it jointly with a log-likelihood on the new incoming chunk.\n",
    "$$\n",
    "L_{new} = const + \\frac{1}{2}(\\theta - \\hat \\theta)^T \\mathbb{H} (\\theta - \\hat \\theta) + log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$\n",
    "$$\n",
    "\\hat \\theta_{new} = argmax ~ L_{new}\n",
    "$$\n",
    "and then we can replace this joint loss (on the incoming data and the surrogate representative of the loss on the \"old\" dataset) by a new quadratic surrogate, so that it can be used later.\n",
    "\n",
    "$$\n",
    "L_{new}(\\theta) \\approx L_{new}(\\hat \\theta_{new}) + \\frac{1}{2}(\\theta - \\hat \\theta_{new})^T \\left[ \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} \\right](\\theta - \\hat \\theta_{new})\n",
    "$$\n",
    "As earlier, we have\n",
    "$$\n",
    "\\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} L_{new} = \\mathbb{H}_d + \\nabla \\nabla_ {\\theta=\\hat \\theta_{new}} log P(\\mathbb{D}_{new} | \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will illustrate the sequential learning approach in a following experiment. Let's take the MNIST dataset and split its training subset in 10 equal parts. After that, we will sequentially learn on them and control how the loss on the validation dataset is behaving. As baselines we will consider two set-ups. Firstly, we will learn a single model using the whole training dataset. That baseline would provide us with an upper-bound for the performance. Secondly, we will sequentally train a single model on these 10 parts, but without using the earlier output as a prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from model import LaplacedModel, get_learning_curve\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "sequential_tasks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning task # 0\n",
      "validation loss: [0.57260001]\n",
      "learning task # 1\n",
      "validation loss: [0.70190001]\n",
      "learning task # 2\n",
      "validation loss: [0.7683]\n",
      "learning task # 3\n",
      "validation loss: [0.8039]\n",
      "learning task # 4\n",
      "validation loss: [0.8143]\n",
      "learning task # 5\n",
      "validation loss: [0.83139998]\n",
      "learning task # 6\n",
      "validation loss: [0.82880002]\n",
      "learning task # 7\n",
      "validation loss: [0.83759999]\n",
      "learning task # 8\n",
      "validation loss: [0.8387]\n",
      "learning task # 9\n",
      "validation loss: [0.84320003]\n"
     ]
    }
   ],
   "source": [
    "model_laplaced = LaplacedModel(4, 0.0, 1.0, True)\n",
    "laplaced_loss_curve = get_learning_curve(model_laplaced, mnist.train, mnist.test, sequential_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57260001], [0.70190001], [0.7683], [0.8039], [0.8143], [0.83139998], [0.82880002], [0.83759999], [0.8387], [0.84320003]]\n"
     ]
    }
   ],
   "source": [
    "print laplaced_loss_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning task # 0\n",
      "validation loss: [0.57260001]\n",
      "learning task # 1\n",
      "validation loss: [0.58840001]\n",
      "learning task # 2\n",
      "validation loss: [0.61210001]\n",
      "learning task # 3\n",
      "validation loss: [0.59939998]\n",
      "learning task # 4\n",
      "validation loss: [0.58579999]\n",
      "learning task # 5\n",
      "validation loss: [0.6081]\n",
      "learning task # 6\n",
      "validation loss: [0.6103]\n",
      "learning task # 7\n",
      "validation loss: [0.63]\n",
      "learning task # 8\n",
      "validation loss: [0.57090002]\n",
      "learning task # 9\n",
      "validation loss: [0.60110003]\n"
     ]
    }
   ],
   "source": [
    "model_naive = LaplacedModel(4, 0.0, 1.0, False)\n",
    "naive_loss_curve = get_learning_curve(model_naive, mnist.train, mnist.test, sequential_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57260001], [0.58840001], [0.61210001], [0.59939998], [0.58579999], [0.6081], [0.6103], [0.63], [0.57090002], [0.60110003]]\n"
     ]
    }
   ],
   "source": [
    "print naive_loss_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6985\n",
      "0.863\n",
      "0.8735\n",
      "0.888\n",
      "0.8875\n",
      "0.8975\n",
      "0.9125\n",
      "0.92\n",
      "0.909\n",
      "0.9145\n"
     ]
    }
   ],
   "source": [
    "upper_bound_model = LaplacedModel(4, 0.0, 1.0, False)\n",
    "upper_bound_curve = upper_bound_model.learn_new_task(mnist.train, mnist.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92650002]\n"
     ]
    }
   ],
   "source": [
    "print upper_bound_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13ae08ed0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncSGFu4iIC/0fDS2PmNRbKusAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b073a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(naive_loss_curve, label='Naive updates')\n",
    "plt.plot(laplaced_loss_curve, label='Laplace updates')\n",
    "plt.plot([upper_bound_curve for _ in naive_loss_curve], label='Upper bound')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While the updated learning is not as good as the upper bound, but we see it's way better than the naive approach. Hence, it's a nice alternative when the it is infeasible/too expensive to learn on the entire dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
